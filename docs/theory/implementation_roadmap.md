# Implementation Roadmap

> **Purpose** ‚Äî describe the state of the rearchitected SSVAE, highlight what is already production-ready, and call out the next focused efforts.  
> **Theory:** [Conceptual Model](conceptual_model.md) ¬∑ [Math Spec](mathematical_specification.md)  
> **Implementation:** [Architecture Guide](../development/architecture.md) ¬∑ [Implementation Guide](../development/implementation.md)

---

## Current Snapshot ¬∑ Nov¬†2025

| Pillar | Status | Key files / notes |
|--------|--------|-------------------|
| Mixture prior with entropy + Dirichlet controls | ‚úÖ shipping | `src/model/ssvae/priors/mixture.py`, `src/model/training/losses.py` (usage penalty + Dirichlet) |
| Component-aware decoder (dense/conv, heteroscedastic variants) | ‚úÖ shipping | `src/model/ssvae/components/decoders.py` |
| œÑ-classifier latent workflow (responsibility-based) | ‚úÖ shipping | `src/model/ssvae/components/tau_classifier.py`, now enabled for **all** mixture-based priors |
| Heteroscedastic decoder + weighted loss | ‚úÖ needs tuning knobs only | `src/model/ssvae/components/decoders.py`, `src/model/training/losses.py` |
| VampPrior (pseudo-input learning, MC-KL) | ‚úÖ shipping | `src/model/ssvae/priors/vamp.py`, network now caches pseudo stats & supports pseudo-LR scaling |
| Geometric MoG (diagnostic/curriculum prior) | ‚úÖ shipping | `src/model/ssvae/priors/geometric_mog.py` |
| OOD scoring via `r √ó œÑ` | üìã ready once experiment wiring added |
| Dynamic label addition / active learning loop | üìã design ready; needs workflow + UX |

Legend: ‚úÖ production-ready ¬∑ ‚ö†Ô∏è needs tuning ¬∑ üìã planned/ready-to-wire

---

## Completed Pillars

### Component-Aware Decoder
- **What**: learns component embeddings `e_c` that specialize the decoder (parallel paths for `z` and `e_c`, merge mid-way). Supports dense/conv + heteroscedastic heads.  
- **Why**: lets mixture components acquire functional roles without forcing latent separation.  
- **Where**: `ComponentAware*Decoder` classes; configured via `use_component_aware_decoder`.  
- **Validation**: ablations show improved recon quality and interpretable per-component outputs.

### Mixture Prior with Diversity Controls
- **What**: `MixtureGaussianPrior` handles `KL_z`, `KL_c`, optional Dirichlet MAP on œÄ, and usage-entropy ‚Äúdiversity reward/punishment‚Äù.  
- **Extras**: learnable œÄ (`config.learnable_pi`) with gradient masking when disabled; metrics surfaced via `compute_loss_and_metrics_v2`.  
- **Diagnostics**: callbacks + `DiagnosticsCollector` export component usage, entropies, œÄ histories.

### œÑ-Classifier & Latent Workflow
- **What**: responsibility-based classifier substitutes the head: accumulates soft counts ‚Üí œÑ-map ‚Üí `p(y|x)=Œ£_c q(c|x)œÑ_{c,y}`.  
- **New in this revision**: any **mixture-based prior** (`mixture`, `vamp`, `geometric_mog`) gets œÑ hooks automatically (`SSVAE.config.is_mixture_based_prior()`), so VampPrior experiments can stay latent-only.  
- **Files**: `src/model/ssvae/components/tau_classifier.py`, trainer hooks in `ssvae/models.py` and `training/trainer.py`.

### Heteroscedastic Decoder
- **What**: decoder predicts `(mean, œÉ)`; losses handle either per-sample (standard) or per-component (mixture) heteroscedasticity.  
- **Status**: stable, just needs experiment-level tuning of `sigma_min/max` and loss scaling.  
- **Files**: decoders + `heteroscedastic_reconstruction_loss()` utilities.

### VampPrior Subsystem
- **What**: pseudo-input prior with Monte Carlo KL. Network now re-encodes pseudo-inputs every forward pass and caches `pseudo_z_mean`/`pseudo_z_log_var` in `EncoderOutput.extras`, so the prior remains stateless.  
- **Training hygiene**: `vamp_pseudo_lr_scale` scales gradients for `params['prior']['pseudo_inputs']` inside the JIT train step (see `_scale_vamp_pseudo_gradients()` in `ssvae/factory.py`).  
- **Features**: random or k-means pseudo init, optional multi-sample KL, uniform œÄ for now.  
- **Status**: production-ready for spatial visualization + component-free decoding.

### Geometric Mixture of Gaussians
- **What**: fixed centers (circle/grid) with analytical KL; acts as a curriculum/debug prior.  
- **Safeguards**: validation enforces grid square counts and warns about induced topology.  
- **Status**: shipping but flagged ‚Äúdiagnostic only‚Äù.

---

## Tooling & Infrastructure

- **Factory + Prior registry** ‚Äî `SSVAEFactory` builds networks, optimizers (with gradient masks), and PriorMode instances; new priors just register via `ssvae/priors/__init__.py`.
- **Loss pipeline** ‚Äî `compute_loss_and_metrics_v2` delegates reconstruction + KL to the active prior and merges œÑ losses, keeping trainer logic agnostic.  
- **Diagnostics** ‚Äî `DiagnosticsCollector` + callbacks capture œÄ/usage histories, component entropies, per-component reconstructions, and latent dumps for 2-D runs.  
- **Experiments** ‚Äî configs live under `use_cases/experiments/configs/`; runners log to timestamped result dirs, feeding dashboards/plots.

---

## Next Focus Areas

1. **OOD & Active Learning Loop**
   - Wire the existing metrics (`max_c r_c`, œÑ certainty) into experiment scripts for acquisition and reporting.
   - Surface `get_ood_score()` and responsibility entropy in the CLI/dashboard.

2. **Dynamic Label Addition**
   - Build workflow that monitors free channels (low usage + low œÑ confidence) and spawns new labels/components when thresholds hit.
   - Update œÑ-classifier persistence / checkpointing to handle label-space expansion.

3. **Prior Research Tracks**
   - Learnable œÄ for VampPrior / hybrid priors (requires extending PriorMode interface with optional state).  
   - Flow-based or hierarchical priors once metrics confirm VampPrior + mixture cover the needed regimes.

---

## File Reference

- **Config / validation** ‚Äî `src/model/ssvae/config.py`
- **Network + prior parameters** ‚Äî `src/model/ssvae/network.py`
- **Priors** ‚Äî `src/model/ssvae/priors/{standard,mixture,vamp,geometric_mog}.py`
- **Losses** ‚Äî `src/model/training/losses.py`
- **Trainer / hooks** ‚Äî `src/model/training/trainer.py`
- **Tau classifier** ‚Äî `src/model/ssvae/components/tau_classifier.py`
- **Diagnostics** ‚Äî `src/model/ssvae/diagnostics.py`
- **Experiments** ‚Äî `use_cases/experiments/‚Ä¶`

Use this roadmap with the architecture + implementation guides to stay aligned with the project‚Äôs invariants while iterating.
