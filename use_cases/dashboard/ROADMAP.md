# Dashboard Integration Roadmap

**Purpose**: Guide the evolution of the dashboard from a basic training interface to a full active learning workbench that leverages the rich experiment infrastructure.

**Context**: This document bridges the gap between the CLI experiment workflow (`use_cases/experiments/`) and the interactive dashboard (`use_cases/dashboard/`), enabling the dashboard to generate experiment-quality outputs and visualizations.

## ğŸ“š Related Documentation

**Before starting work**, review these guides for context:

### Essential Reading
- **[AGENTS.md](../../../AGENTS.md)** - How to navigate the documentation ecosystem and understand system architecture
- **[Developer Guide](docs/DEVELOPER_GUIDE.md)** - Dashboard internals, backend integration points, debugging toolkit, extension workflow
- **[Collaboration Notes](docs/collaboration_notes.md)** - Current system snapshot, debugging playbook, recent work
- **[Dashboard README](README.md)** - Quick start, project structure, routes

### Background Context
- **[Experiments Guide](../../experiments/README.md)** - Experiment outputs (REPORT.md, figures), run directory structure
- **[Conceptual Model](../../../docs/theory/conceptual_model.md)** - Mixture model theory, component specialization, Ï„-classifier, OOD scoring
- **[Mathematical Specification](../../../docs/theory/mathematical_specification.md)** - Formal definitions of responsibilities, Ï„-matrix, uncertainty
- **[Architecture Guide](../../../docs/development/architecture.md)** - Design patterns, service layer, state management

### Implementation References
- **[State Management Plan](docs/dashboard_state_plan.md)** - State architecture, AppStateManager design
- **[Config Metadata](core/config_metadata.py)** - How training parameters are defined and validated
- **[State Models](core/state_models.py)** - Immutable dataclasses representing app state

### Quick References
- **Extension Checklist**: [DEVELOPER_GUIDE.md Â§5](docs/DEVELOPER_GUIDE.md) - Step-by-step workflow for adding features
- **Debugging Playbook**: [collaboration_notes.md Â§Debugging](docs/collaboration_notes.md) - Logs, tests, common issues
- **Backend Touchpoints**: [DEVELOPER_GUIDE.md Â§2](docs/DEVELOPER_GUIDE.md) - Where dashboard interacts with SSVAE backend

---

## Executive Summary

### Current State (As of November 2025)

**âœ… Completed**:
- Service architecture refactoring (Phases 1-3)
  - `TrainingService`, `ModelService`, `LabelingService` abstractions
  - `AppStateManager` eliminates global state
  - Commands use dependency injection (no backward compatibility)
- Mixture model support in training workers
  - Captures `responsibilities` and `pi_values`
  - Enables component specialization analysis
- Basic active learning loop
  - Label samples via click â†’ Train â†’ See updated latent space

**âš ï¸ Stabilization Needed**:
- Training state lifecycle bugs (partially fixed, needs testing)
- UI inconsistencies (epochs input override, page navigation)
- Missing error handling and validation
- Incomplete mixture visualization integration

**âŒ Missing**:
- Experiment-quality outputs from dashboard training
- Ï„-matrix (componentâ†’label mapping) visualizations
- Ï€ evolution tracking across training sessions
- Uncertainty/OOD highlighting for strategic labeling
- REPORT.md generation for dashboard runs
- Component specialization diagnostics

### Vision: Dashboard as Active Learning Workbench

The target system enables this workflow:

1. **Diagnose**: Visualize component specialization, Ï„-matrix, uncertainty regions
2. **Strategize**: Identify high-impact samples using OOD scores, component ambiguity
3. **Label**: Click to label strategically chosen samples
4. **Train**: Run training with real-time component evolution tracking
5. **Analyze**: Review experiment-quality REPORT.md with full diagnostics
6. **Iterate**: Repeat with insights from previous run

**Key Principle**: Dashboard training runs should produce the same rich outputs as CLI experiments, just with interactive labeling in the loop.

---

## Phase 0: Stabilization âœ… COMPLETED

**Goal**: Make the current dashboard reliable and bug-free.

**Status**: âœ… Complete (See `PHASE0_TESTING.md` for verification checklist)

### 0.1 Fix Training State Lifecycle âœ…

**Implementation**:
- âœ… All `state_manager.update_state()` calls verified (commits: `a7dde90`, `414778e`)
- âœ… State transition logging added to `AppStateManager.update_state()` (commit: `9784986`)
- âœ… Enhanced `StartTrainingCommand.validate()` with thread safety check (commit: `9784986`)
- âœ… Consistent use of `update_state()` throughout codebase

**Results**:
- Training lifecycle works: start â†’ stop â†’ start, start â†’ complete â†’ start
- State always returns to IDLE after training
- Clear logging for debugging state issues

### 0.2 Fix UI Inconsistencies âœ…

**Implementation**:
- âœ… Added `prevent_initial_call=True` to action callbacks (commit: `45800ba`)
- âœ… Epochs input investigated - behavior is expected (resets to config on navigation)
- âœ… Input validation present in commands
- âœ… Navigation paths tested and working

**Results**:
- Navigation smooth: home â†’ model â†’ training-hub â†’ config
- Form inputs behave predictably
- No unexpected resets during usage

### 0.3 Add Defensive Error Handling âœ…

**Implementation**:
- âœ… Try-catch around `model.predict_batched()` in both training workers (commit: `9784986`)
- âœ… Shape validation for `responsibilities` and `pi_values` (commit: `9784986`)
- âœ… Null checks audited - all callbacks safe
- âœ… Comprehensive error logging and user messages
- âœ… Robust model loading with architecture mismatch handling (commit: `082cef7`)

**Results**:
- No crashes from prediction failures
- Models with incompatible checkpoints load gracefully
- User-friendly error messages with actionable guidance
- System remains stable after any error

### 0.4 Mixture Model Support âœ…

**Implementation**:
- âœ… `_predict_outputs()` helper with validation (commit: `9784986`)
- âœ… Graceful handling of None responsibilities
- âœ… Shape validation prevents crashes
- âœ… Training hub worker has full mixture support (commit: `2eef358`)

**Results**:
- Mixture models work end-to-end
- Non-mixture models unaffected (responsibilities=None handled)
- Defensive error handling prevents crashes

**Key Commits**:
- `9784986` - Comprehensive error handling and logging
- `414778e` - Fixed state update in clear_hub_terminal
- `082cef7` - Robust model loading with mismatch handling
- `45800ba` - UI consistency improvements
- `2eef358` - Mixture model support in training hub
- `a7dde90` - State update fixes

**Testing**: See `PHASE0_TESTING.md` for comprehensive test checklist

---

## Phase 1: Experiment Integration Foundation

**Goal**: Enable dashboard training runs to generate experiment-quality outputs.

### 1.1 Leverage `generate_dashboard_run()` Infrastructure

**Background**: The function `use_cases/dashboard/core/run_generation.py::generate_dashboard_run()` already exists and creates full experiment outputs. It's called in `CompleteTrainingCommand` but outputs aren't surfaced in the UI.

**See Also**: [Experiments Guide - Run Directory Layout](../../experiments/README.md) for output structure details

**Current Flow**:
```python
# In CompleteTrainingCommand.execute()
run_record = generate_dashboard_run(
    model_id=model_id,
    config=state.active_model.config,
    # ... metrics, diagnostics
)
# run_record created but not used!
```

**Tasks**:
1. Read `use_cases/dashboard/core/run_generation.py` to understand output structure
2. Verify `RunRecord` contains all necessary metadata (path to REPORT.md, figures, etc.)
3. Add `run_record` to `ModelState.runs` tuple in `CompleteTrainingCommand`
4. Update `ModelState` schema to include link to latest run's REPORT.md

**Design Decision**:
- Keep using `generate_dashboard_run()` - don't duplicate experiment infrastructure
- Store run records in `ModelState.runs` for UI access
- Link to generated REPORT.md instead of duplicating content

**Acceptance Criteria**:
- After training, `ModelState.runs` contains new `RunRecord`
- `RunRecord` includes paths to REPORT.md and figures
- Training hub shows link to latest run's REPORT.md

### 1.2 Surface Experiment Outputs in Training Hub

**Goal**: Make REPORT.md and figures accessible from the UI.

**Tasks**:
1. Add "View Latest Report" button to training hub
2. Create new route `/model/{id}/run/{run_id}` to display REPORT.md
3. Render markdown with embedded image references
4. Add "Recent Runs" section showing last 5 runs with thumbnails

**UI Design**:
```
Training Hub
â”œâ”€â”€ Training Controls (existing)
â”œâ”€â”€ Loss Curves (existing)
â””â”€â”€ Recent Runs (NEW)
    â”œâ”€â”€ Run 1: 20251116_143022 [View Report] [Best val: 0.023]
    â”œâ”€â”€ Run 2: 20251116_122801 [View Report] [Best val: 0.031]
    â””â”€â”€ ...
```

**Acceptance Criteria**:
- User can click "View Report" to see full REPORT.md
- Images in report render correctly
- Run metadata shows in UI (timestamp, epochs, best metrics)

### 1.3 Expose Mixture Diagnostics in UI

**Goal**: Surface component specialization information during and after training.

**Background**: The experiment workflow generates:
- `latent_by_component.png`: Latent scatter colored by component assignment
- `responsibility_histogram.png`: Distribution of component ownership
- `tau_matrix_heatmap.png`: Componentâ†’label mapping visualization
- Ï€ evolution plots

**See Also**:
- [Conceptual Model Â§How-We-Classify](../../../docs/theory/conceptual_model.md) - Component specialization theory
- [Experiments Guide Â§Channel-Wise Latent Diagnostic](../../experiments/README.md) - Visualization details
- [Mathematical Specification](../../../docs/theory/mathematical_specification.md) - Formal definition of Ï„-matrix

**Tasks**:
1. Create new "Component Analysis" tab in training hub
2. Display Ï„-matrix heatmap (if mixture model)
3. Show latent space colored by component (not just by label)
4. Add toggle: "Color by Label" vs "Color by Component"
5. Display current Ï€ values as bar chart

**UI Layout**:
```
Component Analysis Tab
â”œâ”€â”€ Component Assignment (Latent Space)
â”‚   â””â”€â”€ Scatter plot colored by argmax(responsibilities)
â”œâ”€â”€ Ï„-Matrix Heatmap
â”‚   â””â”€â”€ Shows which components map to which labels
â”œâ”€â”€ Mixture Weights (Ï€)
â”‚   â””â”€â”€ Bar chart of current component probabilities
â””â”€â”€ Responsibility Distribution
    â””â”€â”€ Histogram of max responsibilities (certainty measure)
```

**Acceptance Criteria**:
- Mixture models show all component visualizations
- Non-mixture models show message "Not a mixture model"
- Visualizations update after each training run
- Can toggle between label-colored and component-colored latent views

**Estimated Duration**: 3-4 days

---

## Phase 2: Active Learning Intelligence

**Goal**: Guide users to label strategically impactful samples.

### 2.1 Implement OOD Scoring

**Background**: The conceptual model defines OOD score as:
```
OOD(x) = 1 - max_c [r_c(x) Â· max_y Ï„_{c,y}]
```

This identifies points not well-owned by any labeled component.

**See Also**: [Conceptual Model Â§How-We-Classify](../../../docs/theory/conceptual_model.md) - OOD scoring derivation and intuition

**Tasks**:
1. Add `compute_ood_scores()` to `ModelState` (uses responsibilities + Ï„)
2. Store OOD scores in `DataState.ood_scores` array
3. Update `_build_hover_metadata()` to include OOD score
4. Color latent scatter by OOD score (high OOD = red, low = blue)

**Validation**:
- Unlabeled, ambiguous regions should have high OOD scores
- Well-classified, certain regions should have low OOD scores
- Samples from labeled classes within cluster cores: OOD â‰ˆ 0

**Acceptance Criteria**:
- Latent visualization has "Color by OOD Score" mode
- Hover shows OOD score for each point
- High-OOD regions are visually obvious (red highlight)

### 2.2 Strategic Labeling Suggestions

**Goal**: Automatically suggest which samples to label next.

**Strategies**:
1. **High OOD**: Label samples far from labeled component distributions
2. **Component Ambiguity**: Label samples with low `max(responsibilities)`
3. **Label Diversity**: Ensure all classes have minimum representation
4. **Boundary Samples**: Points near decision boundaries (low certainty)

**Tasks**:
1. Implement `suggest_labeling_candidates()` function
2. Takes `DataState`, `Ï„-matrix`, current label counts
3. Returns ranked list of sample indices with rationale
4. Add "Suggested Labels" panel to UI
5. Clicking suggestion centers view on that point

**UI Design**:
```
Labeling Suggestions Panel
â”œâ”€â”€ Strategy: [High OOD â–¼]
â”œâ”€â”€ Top Candidates:
â”‚   â”œâ”€â”€ Sample 1247 - OOD: 0.87, Component: uncertain
â”‚   â”œâ”€â”€ Sample 0892 - OOD: 0.79, Near boundary
â”‚   â””â”€â”€ Sample 2103 - OOD: 0.74, Class 7 underrepresented
â””â”€â”€ [Apply Label] [View Sample] [Skip]
```

**Acceptance Criteria**:
- Suggestions change based on selected strategy
- Clicking suggestion highlights point in latent space
- Can label directly from suggestion panel
- Suggestions update after labeling/training

### 2.3 Track Ï€ Evolution Across Sessions

**Goal**: Show how mixture weights evolve with iterative labeling/training.

**Background**: The experiment workflow saves `pi_history.npy` showing Ï€ evolution during training. We need session-level tracking across multiple training runs.

**Tasks**:
1. Extend `RunRecord` to include `pi_final` values
2. Create `MixtureEvolutionHistory` in `ModelState`
3. After each run, append (timestamp, Ï€_values) to history
4. Add "Ï€ Evolution" plot showing multi-run trajectory
5. Highlight when components become specialized (low entropy)

**Visualization**:
```
Ï€ Evolution (Multi-Run View)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 1.0 â”¤     Component 0 â”€â”€â”€â”€â”€â”€
     â”‚     Component 1 â”€ â”€ â”€
 Ï€   â”‚     Component 2 Â·Â·Â·Â·Â·Â·
     â”‚
 0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     Run 1   Run 2   Run 3   Run 4
     (0 labels) (10 labels) (25 labels)
```

**Acceptance Criteria**:
- Can see Ï€ trajectory across multiple training runs
- Runs annotated with label count at that point
- Clear visual when a component "locks in" to a label

**Estimated Duration**: 4-5 days

---

## Phase 3: Advanced Diagnostics

**Goal**: Replicate full experiment diagnostics in dashboard.

### 3.1 Real-Time Ï„-Matrix Updates

**Current**: Ï„-matrix computed once per training run
**Target**: Live updates during training via `TrainerLoopHooks`

**Tasks**:
1. Modify `DashboardMetricsCallback` to capture Ï„ updates per epoch
2. Send Ï„-matrix snapshots via metrics queue
3. Update UI to show "current" vs "starting" Ï„-matrix
4. Animate transitions (optional, nice-to-have)

**Acceptance Criteria**:
- Ï„-matrix updates in real-time during training
- Can see componentâ†’label assignments converging
- Final Ï„-matrix matches what's saved in diagnostics

### 3.2 Per-Component Latent Views

**Background**: Experiments generate `channel_latents/` with per-component views where opacity = responsibility.

**Tasks**:
1. Add component selector dropdown
2. For selected component, dim points with low responsibility
3. Opacity = `responsibilities[:, selected_component]`
4. Highlight high-responsibility regions

**UI**:
```
Component View: [Component 3 â–¼]
[Latent space where Component 3 owns points appears bright,
 other points are transparent/dim]
```

**Acceptance Criteria**:
- Can view each component's "territory" clearly
- High-responsibility regions stand out
- Works for all mixture model types

### 3.3 Uncertainty Decomposition

**Goal**: Separate aleatoric (data noise) from epistemic (model uncertainty).

**Background**: Heteroscedastic decoder provides ÏƒÂ²(x) for aleatoric uncertainty.

**Tasks**:
1. Store ÏƒÂ² values in `DataState.aleatoric_uncertainty`
2. Compute epistemic uncertainty from responsibility entropy: `-Î£ r_c log r_c`
3. Add "Uncertainty Analysis" visualization
4. Color latent space by uncertainty type

**Visualization**:
```
Uncertainty Decomposition
â”œâ”€â”€ Aleatoric (ÏƒÂ²): Inherent data noise
â”‚   â””â”€â”€ [Latent scatter colored by ÏƒÂ²]
â”œâ”€â”€ Epistemic (H(r)): Model uncertainty about component
â”‚   â””â”€â”€ [Latent scatter colored by entropy]
â””â”€â”€ Combined Risk
    â””â”€â”€ [2D heatmap: epistemic Ã— aleatoric]
```

**Acceptance Criteria**:
- Can distinguish noisy data from uncertain model regions
- High epistemic + low aleatoric = needs more labels
- High aleatoric = inherently ambiguous data

**Estimated Duration**: 5-6 days

---

## Phase 4: Production Readiness

**Goal**: Make dashboard robust for extended research sessions.

### 4.1 Model Versioning & Checkpointing

**Tasks**:
1. Track model version across training runs
2. Enable rollback to previous checkpoint
3. Add "Compare Runs" view (side-by-side Ï„-matrices)
4. Export model with full diagnostics

### 4.2 Session Persistence

**Tasks**:
1. Save dashboard state to disk periodically
2. Restore labeling progress after reload
3. Cache latent embeddings for fast page loads
4. Add "Resume Session" on dashboard start

### 4.3 Batch Labeling Operations

**Tasks**:
1. Multi-select mode for labeling
2. "Label all in region" via lasso selection
3. Undo/redo for labeling actions
4. Import/export labels as CSV

### 4.4 Performance Optimization

**Tasks**:
1. Lazy-load large visualizations
2. Reduce data sent to frontend (downsample scatter plots)
3. Server-side rendering for complex plots
4. Add loading spinners with progress

**Estimated Duration**: 6-8 days

---

## Architecture Decisions & Patterns

### Key Principles

1. **Reuse Experiment Infrastructure**: Don't duplicate visualization/metrics code
   - âœ… Use `generate_dashboard_run()` for outputs
   - âœ… Leverage existing metrics registry
   - âœ… Call experiment plotting functions directly

2. **Service Layer for Domain Logic**: Keep commands thin
   - âœ… Services handle model operations
   - âœ… Commands orchestrate services
   - âœ… No business logic in callbacks

3. **Immutable State with Explicit Updates**: Never mutate `AppState` directly
   - âœ… Always use `state.with_active_model(updated_model)`
   - âœ… Call `state_manager.update_state()` to persist
   - âŒ Never assign `state_manager.state = ...`

4. **Mixture-Aware by Default**: All code should handle both mixture and non-mixture models
   - âœ… Check `if responsibilities is not None:` before using
   - âœ… Gracefully degrade for non-mixture models
   - âœ… Use `_predict_outputs()` helper for consistent handling

### Critical Code Patterns

#### Pattern 1: Updating Model State

```python
# âœ… CORRECT
with state_manager.state_lock:
    if state_manager.state.active_model:
        updated_model = state_manager.state.active_model.with_training(
            state=TrainingState.IDLE,
            # ... other updates
        )
        state_manager.update_state(
            state_manager.state.with_active_model(updated_model)
        )

# âŒ WRONG - direct assignment doesn't persist
state_manager.state = new_state
```

#### Pattern 2: Mixture Model Prediction

```python
# âœ… CORRECT - handles both mixture and non-mixture
def _predict_outputs(model, data):
    try:
        mixture_mode = bool(model.config.is_mixture_based_prior())
    except AttributeError:
        mixture_mode = False

    if mixture_mode:
        latent, recon, preds, cert, resp, pi = model.predict_batched(
            data, return_mixture=True
        )
        return latent, recon, preds, cert, resp, pi

    latent, recon, preds, cert = model.predict_batched(data)
    return latent, recon, preds, cert, None, None

# âŒ WRONG - crashes on non-mixture models
latent, recon, preds, cert, resp, pi = model.predict_batched(data, return_mixture=True)
```

#### Pattern 3: Null-Safe Visualization

```python
# âœ… CORRECT
if responsibilities is not None:
    # Show component-colored view
    colors = responsibilities.argmax(axis=1)
else:
    # Fall back to label-colored view
    colors = labels

# âŒ WRONG - crashes when responsibilities is None
colors = responsibilities.argmax(axis=1)
```

### File Organization

```
use_cases/dashboard/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ state_manager.py       # AppStateManager (all state operations)
â”‚   â”œâ”€â”€ state_models.py        # Immutable state dataclasses
â”‚   â”œâ”€â”€ commands.py            # Command pattern (orchestration)
â”‚   â”œâ”€â”€ model_manager.py       # File I/O for models
â”‚   â””â”€â”€ run_generation.py      # Experiment output generation
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ training_service.py    # Training execution
â”‚   â”œâ”€â”€ model_service.py       # Model CRUD
â”‚   â”œâ”€â”€ labeling_service.py    # Label persistence
â”‚   â””â”€â”€ container.py           # Dependency injection
â”œâ”€â”€ callbacks/
â”‚   â”œâ”€â”€ training_callbacks.py       # Main dashboard training
â”‚   â”œâ”€â”€ training_hub_callbacks.py   # Training hub page
â”‚   â”œâ”€â”€ labeling_callbacks.py       # Labeling interactions
â”‚   â””â”€â”€ visualization_callbacks.py  # Plot updates
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ home.py               # Model selection
â”‚   â”œâ”€â”€ training_hub.py       # Training workbench
â”‚   â””â”€â”€ layouts.py            # Main dashboard layout
â””â”€â”€ utils/
    â”œâ”€â”€ visualization.py       # Plotting utilities
    â””â”€â”€ training_callback.py   # JAXâ†’Dashboard bridge
```

### Data Flow

```
User Action (Click/Train)
    â†“
Dash Callback
    â†“
Create Command (with params)
    â†“
dispatcher.execute(command)
    â†“
Command.validate(state, services) â†’ Error or None
    â†“
Command.execute(state, services) â†’ (new_state, message)
    â†“
state_manager.update_state(new_state)
    â†“
UI Updates (via polling or reactive callbacks)
```

### Testing Strategy

1. **Unit Tests**: Services and commands in isolation
2. **Integration Tests**: Full command execution with mock state
3. **UI Tests**: Critical user flows (label â†’ train â†’ view results)
4. **Mixture Model Tests**: Verify both mixture and non-mixture paths

---

## Migration Guide for Next Session

### Quick Start Checklist

When resuming work:

1. **Read this document** to understand vision and current status
2. **Check recent commits** for any changes since this was written
3. **Run the dashboard** and test basic flow:
   - Load model
   - Label a few samples
   - Start training
   - Stop training
   - Start again (should work without errors)
4. **Identify current phase** based on what's working/broken
5. **Pick next task** from the appropriate phase

### Common Pitfalls to Avoid

1. **Don't bypass state_manager**: Always use `update_state()`, never direct assignment
2. **Don't assume mixture model**: Check `responsibilities is not None` before using
3. **Don't duplicate experiment code**: Import and call existing functions
4. **Don't modify state during read**: Use `with state_lock:` for snapshots only
5. **Don't skip validation**: Commands should validate before executing

### Key Files to Reference

**Core Documentation**:
- **[AGENTS.md](../../../AGENTS.md)** - Documentation navigation guide
- **[Developer Guide](docs/DEVELOPER_GUIDE.md)** - Dashboard architecture, backend touchpoints
- **[Architecture](../../../docs/development/architecture.md)** - Design patterns, service layer
- **[Conceptual Model](../../../docs/theory/conceptual_model.md)** - Mixture models, Ï„-classifier, OOD theory

**Implementation Details**:
- **[Experiments Guide](../../experiments/README.md)** - CLI workflow, REPORT.md structure
- **[State Models](core/state_models.py)** - Immutable dataclasses (AppState, ModelState, etc.)
- **[Config Metadata](core/config_metadata.py)** - Training parameter definitions
- **[Collaboration Notes](docs/collaboration_notes.md)** - Recent work, debugging playbook

### Debugging Tips

1. **Training stuck**: Check `state.active_model.training.state` - should be IDLE when not training
2. **Missing mixture data**: Verify `_predict_outputs()` is called, not direct `model.predict()`
3. **UI not updating**: Check metrics queue is being drained in polling callback
4. **State inconsistencies**: Add logging to `AppStateManager.update_state()`

---

## Success Metrics

### Phase 0 (Stabilization)
- âœ… Can complete 5 trainâ†’stopâ†’train cycles without errors
- âœ… All mixture model tests pass
- âœ… No silent failures in logs

### Phase 1 (Experiment Integration)
- âœ… REPORT.md generated for every training run
- âœ… Can view past run reports from UI
- âœ… Mixture visualizations appear in experiment browser

### Phase 2 (Active Learning)
- âœ… OOD scores correlate with uncertainty
- âœ… Suggested labels guide user to high-impact samples
- âœ… Ï€ evolution shows component specialization over time

### Phase 3 (Advanced Diagnostics)
- âœ… Real-time Ï„-matrix updates during training
- âœ… Per-component latent views isolate specialization
- âœ… Can decompose uncertainty into aleatoric/epistemic

### Phase 4 (Production)
- âœ… Session persists across browser refreshes
- âœ… Can handle 10+ training runs without performance degradation
- âœ… Batch labeling speeds up curriculum building

---

## Appendix: Technical Debt Tracking

### Known Issues (To Fix During Stabilization)

1. **FAST_DASHBOARD_MODE bypasses real predictions**: Should be deprecated once performance is optimized
2. **Labels stored in CSV and in-memory**: Single source of truth should be LabelingService
3. **Global `dashboard_state` module import**: Should pass `state_manager` explicitly where possible
4. **Polling callbacks**: Could be replaced with WebSocket for lower latency
5. **Mixture data not always persisted**: `responsibilities` and `pi_values` not saved to disk

### Future Enhancements (Beyond This Roadmap)

1. **Multi-user support**: Track per-user labeling sessions
2. **Distributed training**: Offload training to backend workers
3. **Curriculum scheduling**: Auto-adjust hyperparameters based on label distribution
4. **Export to paper**: Generate LaTeX-ready figures from dashboard
5. **Model comparison**: Side-by-side view of multiple model variants

---

**Last Updated**: November 2025
**Status**: Phase 0 in progress
**Next Milestone**: Complete stabilization, begin Phase 1
