
experiment:
  name: "prior_mixture"
  description: "Test"
  tags: []
# ============================================================================
# DATA CONFIGURATION
# ============================================================================
data:
  dataset: "mnist"                  # Currently only MNIST is supported
  num_samples: 3333               # Total samples from dataset
  num_labeled: 0                  # Labeled samples (rest are unlabeled)
  seed: 42                          # Random seed for data sampling
  dataset_variant: "mnist"          # "mnist" (default) or "digits" 
# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
model:
  # ──────────────────────────────────────────────────────────────────────────
  # Core Architecture
  # ──────────────────────────────────────────────────────────────────────────
  num_classes: 10                   # Output classes (10 for MNIST digits)
  latent_dim: 2                     # Latent space dimensions (2D for visualization)
  latent_layout: "decentralized"    # "shared" (legacy) or "decentralized" (one z per component)
  encoder_type: "conv"              # "conv", "dense" 
  decoder_type: "conv"              
  classifier_type: "dense"          
  hidden_dims: [256, 128, 64]       # Layer sizes for dense encoder/decoder
                                    # (ignored when using conv architecture)
  # ──────────────────────────────────────────────────────────────────────────
  # Loss Configuration
  # ──────────────────────────────────────────────────────────────────────────
  reconstruction_loss: "bce"        # "bce", "mse" 
  recon_weight: 1.0                 # Reconstruction term weight (1.0 for BCE, 500 for MSE)
  kl_weight: 1.0                    # KL divergence weight (standard VAE β=1)
  label_weight: 1.0                 # Classification loss weight 
  # ──────────────────────────────────────────────────────────────────────────
  # Training Configuration
  # ──────────────────────────────────────────────────────────────────────────
  learning_rate: 0.001              
  batch_size: 128                   
  max_epochs: 100                    
  patience: 10                      
  val_split: 0.1                    
  random_seed: 42                   
  grad_clip_norm: 5.0               # Gradient clipping (null to disable)
  weight_decay: 0.0001              # L2 regularization via AdamW
  dropout_rate: 0.2                 # Dropout in classifier
  monitor_metric: "loss"            # Metric for early stopping: "loss", "val_loss", or "classification_loss"
  # ──────────────────────────────────────────────────────────────────────────
  # Prior Configuration (Core Design Choice)
  # ──────────────────────────────────────────────────────────────────────────
  prior_type: "mixture"             # Options: "standard", "mixture", "vamp", "geometric_mog"
                                    # - "standard": Single Gaussian N(0,I)
                                    # - "mixture": Learned mixture of Gaussians 
                                    # - "geometric_mog": Fixed geometric arrangement
                                    # - "vamp": Spatial clustering via pseudo-inputs
  num_components: 10                # Number of mixture components (K)
                                    # Should be >= num_classes for multimodal support
  # Component specialization (mixture/vamp/geometric_mog only)
  use_component_aware_decoder: false # Decoder variant for component embeddings:
                                    # - false: concat [z, e_c] as input (shared decoder weights)
                                    # - true: separate pathways z→Dense, e_c→Dense, then merge
                                    # Both receive embeddings; true enables specialization
  component_embedding_dim: 32        # Embedding size for component conditioning (4-16)
  use_film_decoder: true           # FiLM conditioning (dense/conv decoder)
  # Discrete routing controls (mixture / geometric)
  use_gumbel_softmax: true         # If true, sample c via Gumbel-Softmax for decentralization experiments
  use_straight_through_gumbel: true # Straight-through one-hot for decoder selection
  gumbel_temperature: 2.0           # Initial temperature for Gumbel-Softmax
  gumbel_temperature_min: 0.5
  gumbel_temperature_anneal_epochs: 100
  # VampPrior-specific settings (only used when prior_type="vamp")
  # VampPrior learns K pseudo-inputs and defines p(z) = Σ π_k q(z|u_k)
  vamp_pseudo_init_method: "kmeans" # Pseudo-input initialization: "kmeans" or "random"
  vamp_num_samples_kl: 1            # Monte Carlo samples for KL estimation (1, 5, or 10)
                                    # Higher = lower variance but slower
  vamp_pseudo_lr_scale: 0.1         # Learning rate multiplier for pseudo-inputs (0.05-0.2)
  # Geometric MoG settings (only used when prior_type="geometric_mog")
  # Fixed geometric arrangement for debugging/visualization
  # For details, see geometric_grid_mnist.yaml
  geometric_arrangement: "circle"   # "circle" (any K) or "grid" (K must be perfect square)
  geometric_radius: 2.0             # Spatial radius (1.5-3.0)

  # Mixture/VampPrior regularization (not used with standard/geometric_mog)
  kl_c_weight: 0.05                  # Component assignment KL weight
  kl_c_anneal_epochs: 100            # Anneal kl_c_weight from 0→1 over N epochs
                                    # (0 = no annealing, start at full weight)
  component_diversity_weight: -15.00 # Entropy reward for diverse component usage
                                    # NEGATIVE = encourage diversity (prevents collapse)
                                    # POSITIVE = discourage diversity (not recommended)
  learnable_pi: false
  dirichlet_alpha: null             # Dirichlet prior strength (null = disabled)
  dirichlet_weight: 0.05            # Dirichlet regularization weight
  # ──────────────────────────────────────────────────────────────────────────
  # Classification Strategy
  # ──────────────────────────────────────────────────────────────────────────
  # The model supports two classification approaches:
  # 1. τ-classifier: Latent-only via responsibilities (mixture/vamp only)
  # 2. Standard: Separate classifier head on latent z
  use_tau_classifier: false          # Use τ-based latent-only classification
                                    # Requires: num_components >= num_classes
                                    # Works with: mixture, vamp priors
  tau_smoothing_alpha: 1.0          # Laplace smoothing for τ counts (prevents zeros)
  # ──────────────────────────────────────────────────────────────────────────
  # Uncertainty Estimation
  # ──────────────────────────────────────────────────────────────────────────
  use_heteroscedastic_decoder: true # Learn per-image reconstruction variance
                                    # Outputs: (mean, sigma) for uncertainty quantification
  sigma_min: 0.05                   # Minimum variance (prevents collapse)
  sigma_max: 0.5                    # Maximum variance (prevents explosion)
  # ──────────────────────────────────────────────────────────────────────────
  # Advanced Options 
  # ──────────────────────────────────────────────────────────────────────────
  # Component gating (for large K)
  top_m_gating: 0                   # Use only top-M components (0 = use all) (not yet implemented)
  soft_embedding_warmup_epochs: 10  # Soft-weighted embeddings for N epochs (0 = hard from start)
  # Contrastive learning (experimental)
  use_contrastive: false            # Enable contrastive loss (not yet implemented)
  contrastive_weight: 0.0           # Contrastive loss weight
  # Diagnostics
  mixture_history_log_every: 1      # Log π and usage every N epochs (0 = disable)
