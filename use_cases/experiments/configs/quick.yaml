# ============================================================================
# Default Configuration - Semi-Supervised VAE with Mixture Prior
# ============================================================================
# This is the recommended starting point for most experiments.
# It uses proven defaults and the mixture prior architecture from the
# conceptual model (see docs/theory/conceptual_model.md).
#
# What you get:
# - Mixture prior with K=10 components for multimodal data
# - Component-aware decoder for channel specialization
# - τ-classifier for latent-only classification
# - Convolutional architecture for MNIST images
# - Heteroscedastic decoder for uncertainty estimation
# - Sensible hyperparameters validated through experimentation
#
# To customize:
# 1. Copy this file: cp default.yaml my_experiment.yaml
# 2. Edit the parameters you want to change
# 3. Run: poetry run python use_cases/experiments/run_experiment.py --config my_experiment.yaml
#
# For other prior types (VampPrior, Geometric MoG), see:
# - vamp_best_mnist.yaml (spatial clustering via learned pseudo-inputs)
# - geometric_grid_mnist.yaml (fixed geometric arrangement for debugging)
# ============================================================================

experiment:
  name: "prior_mixture"
  description: "Test"
  tags: []






# ============================================================================
# DATA CONFIGURATION
# ============================================================================
data:
  dataset: "mnist"                  # Currently only MNIST is supported
  num_samples: 3333               # Total samples from dataset
  num_labeled: 333                  # Labeled samples (rest are unlabeled)
  seed: 42                          # Random seed for data sampling
  dataset_variant: "mnist"          # "mnist" (default) or "digits" to force fallback






# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
model:





  # ──────────────────────────────────────────────────────────────────────────
  # Core Architecture
  # ──────────────────────────────────────────────────────────────────────────
  num_classes: 10                   # Output classes (10 for MNIST digits)
  latent_dim: 2                     # Latent space dimensions (2D for visualization)
  latent_layout: "decentralized"            # "shared" (legacy) or "decentralized" (one z per component)
  
  encoder_type: "conv"              # "conv" for images, "dense" for tabular
  decoder_type: "conv"              # Should match encoder_type
  classifier_type: "dense"          # Classifier architecture (currently only "dense")
  
  hidden_dims: [256, 128, 64]       # Layer sizes for dense encoder/decoder
                                    # (ignored when using conv architecture)

  # ──────────────────────────────────────────────────────────────────────────
  # Loss Configuration
  # ──────────────────────────────────────────────────────────────────────────
  reconstruction_loss: "bce"        # "bce" for binary/MNIST, "mse" for natural images
  recon_weight: 1.0                 # Reconstruction term weight (1.0 for BCE, 500 for MSE)
  kl_weight: 1.0                    # KL divergence weight (standard VAE β=1)
  label_weight: 1.0                 # Classification loss weight (currently unused)

  # ──────────────────────────────────────────────────────────────────────────
  # Training Configuration
  # ──────────────────────────────────────────────────────────────────────────
  learning_rate: 0.001              # Adam optimizer learning rate
  batch_size: 128                   # Samples per batch
  max_epochs: 100                    # Maximum training epochs
  patience: 20                      # Early stopping patience (epochs without improvement)
  val_split: 0.1                    # Validation set fraction
  
  random_seed: 42                   # Random seed for reproducibility
  grad_clip_norm: 5.0               # Gradient clipping (null to disable)
  weight_decay: 0.0001              # L2 regularization via AdamW
  dropout_rate: 0.2                 # Dropout in classifier
  
  monitor_metric: "loss"            # Metric for early stopping: "loss", "val_loss", or "classification_loss"







  # ──────────────────────────────────────────────────────────────────────────
  # Prior Configuration (Core Design Choice)
  # ──────────────────────────────────────────────────────────────────────────
  # The prior determines how latent space is structured.
  # See docs/theory/conceptual_model.md for design rationale.
  
  prior_type: "mixture"             # Options: "standard", "mixture", "vamp", "geometric_mog"
                                    # - "mixture": Learned mixture of Gaussians (RECOMMENDED)
                                    # - "vamp": Spatial clustering via pseudo-inputs
                                    # - "standard": Single Gaussian N(0,I)
                                    # - "geometric_mog": Fixed geometric arrangement (diagnostic)
  
  num_components: 10                # Number of mixture components (K)
                                    # Should be >= num_classes for multimodal support
                                    # Typical range: 10-20 for MNIST

  # Component specialization (mixture/vamp/geometric_mog only)
  use_component_aware_decoder: true # Decoder variant for component embeddings:
                                    # - false: concat [z, e_c] as input (shared decoder weights)
                                    # - true: separate pathways z→Dense, e_c→Dense, then merge
                                    # Both receive embeddings; true enables specialization
  component_embedding_dim: 32        # Embedding size for component conditioning (4-16)
  use_film_decoder: true           # FiLM conditioning (dense/conv decoder)

  # Discrete routing controls (mixture / geometric)
  use_gumbel_softmax: true         # If true, sample c via Gumbel-Softmax for decentralization experiments
  use_straight_through_gumbel: true # Straight-through one-hot for decoder selection
  gumbel_temperature: 2.0           # Initial temperature for Gumbel-Softmax
  gumbel_temperature_min: 0.5
  gumbel_temperature_anneal_epochs: 80

  # VampPrior-specific settings (only used when prior_type="vamp")
  # VampPrior learns K pseudo-inputs and defines p(z) = Σ π_k q(z|u_k)
  # For details, see vamp_best_mnist.yaml or docs/theory/implementation_roadmap.md
  vamp_pseudo_init_method: "kmeans" # Pseudo-input initialization: "kmeans" or "random"
                                    # "kmeans" recommended (20-30% faster convergence)
  vamp_num_samples_kl: 1            # Monte Carlo samples for KL estimation (1, 5, or 10)
                                    # Higher = lower variance but slower
  vamp_pseudo_lr_scale: 0.1         # Learning rate multiplier for pseudo-inputs (0.05-0.2)

  # Geometric MoG settings (only used when prior_type="geometric_mog")
  # Fixed geometric arrangement for debugging/visualization
  # For details, see geometric_grid_mnist.yaml
  geometric_arrangement: "circle"   # "circle" (any K) or "grid" (K must be perfect square)
  geometric_radius: 2.0             # Spatial radius (1.5-3.0)

  # Mixture/VampPrior regularization (not used with standard/geometric_mog)
  kl_c_weight: 0.15                  # Component assignment KL weight
  kl_c_anneal_epochs: 100            # Anneal kl_c_weight from 0→1 over N epochs
                                    # (0 = no annealing, start at full weight)
  
  component_diversity_weight: -5.00 # Entropy reward for diverse component usage
                                    # NEGATIVE = encourage diversity (prevents collapse)
                                    # POSITIVE = discourage diversity (not recommended)
                                    # Typical range: -0.05 to -0.15
  

  learnable_pi: true
  dirichlet_alpha: 0.3             # Dirichlet prior strength (null = disabled)
                                    # When enabled, try values 2.0-5.0
  dirichlet_weight: 0.15            # Dirichlet regularization weight






  # ──────────────────────────────────────────────────────────────────────────
  # Classification Strategy
  # ──────────────────────────────────────────────────────────────────────────
  # The model supports two classification approaches:
  # 1. τ-classifier: Latent-only via responsibilities (mixture/vamp only)
  # 2. Standard: Separate classifier head on latent z
  
  use_tau_classifier: true          # Use τ-based latent-only classification
                                    # Requires: num_components >= num_classes
                                    # Works with: mixture, vamp priors
  tau_smoothing_alpha: 1.0          # Laplace smoothing for τ counts (prevents zeros)






  # ──────────────────────────────────────────────────────────────────────────
  # Uncertainty Estimation
  # ──────────────────────────────────────────────────────────────────────────
  use_heteroscedastic_decoder: true # Learn per-image reconstruction variance
                                    # Outputs: (mean, sigma) for uncertainty quantification
  sigma_min: 0.05                   # Minimum variance (prevents collapse)
  sigma_max: 0.5                    # Maximum variance (prevents explosion)






  # ──────────────────────────────────────────────────────────────────────────
  # Advanced Options (Usually Don't Need to Change)
  # ──────────────────────────────────────────────────────────────────────────
  
  # Component gating (for large K)
  top_m_gating: 0                   # Use only top-M components (0 = use all)
  soft_embedding_warmup_epochs: 10  # Soft-weighted embeddings for N epochs (0 = hard from start)
  
  # Contrastive learning (experimental)
  use_contrastive: false            # Enable contrastive loss
  contrastive_weight: 0.0           # Contrastive loss weight
  
  # Diagnostics
  mixture_history_log_every: 1      # Log π and usage every N epochs (0 = disable)

# ============================================================================
# EXPECTED BEHAVIOR
# ============================================================================
# With these settings on MNIST, you should expect:
#
# Training:
# - Converges in 30-40 epochs (with early stopping)
# - Loss curves smooth with mixture regularization
#
# Performance:
# - Classification accuracy: 85-95% (depends on num_labeled)
# - Active components: 7-9 out of 10 (due to diversity regularization)
# - Reconstruction quality: Clear digit shapes
#
# Diagnostics:
# - K_eff (effective components): 7-9
# - Responsibility confidence: 0.85-0.95
# - No posterior collapse (all components used)
# - Clean separation in 2D latent space visualization
#
# Failure modes to watch for:
# - Component collapse: Increase component_diversity_weight (more negative)
# - Poor classification: Check num_labeled is sufficient (>= 50)
# - Unstable training: Reduce learning_rate or enable kl_c annealing
# ============================================================================
