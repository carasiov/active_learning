# Demonstration config: decentralized mixture of VAEs (one z per component) with Gumbel-Softmax routing.
#
# Goal:
# - Each component owns its own 2-D latent plane (latent_layout: decentralized)
# - Routing uses Gumbel-Softmax + straight-through for hard selection into the decoder
# - Decoder is component-aware (conv) so conditioning is functional, not just concatenation
#
# Run:
#   poetry run python use_cases/experiments/run_experiment.py --config use_cases/experiments/configs/decentralized_mixture.yaml

experiment:
  name: "decentralized-mixture"
  description: "Mixture-of-VAEs with per-component latents and Gumbel routing"
  tags: ["mixture", "decentralized", "gumbel"]

data:
  dataset: "mnist"
  num_samples: 2000
  num_labeled: 0
  seed: 42
  dataset_variant: "mnist"

model:
  # Core architecture
  num_classes: 10
  latent_dim: 2
  latent_layout: "decentralized"       # One latent per component
  encoder_type: "conv"
  decoder_type: "conv"
  classifier_type: "dense"
  hidden_dims: [256, 128, 64]

  # Prior + routing
  prior_type: "mixture"
  num_components: 10
  use_gumbel_softmax: true
  use_straight_through_gumbel: true
  gumbel_temperature: 1.0

  # Decoder conditioning
  use_component_aware_decoder: true
  use_film_decoder: false              # FiLM is dense-only; conv path uses component-aware
  component_embedding_dim: 16

  # Loss weights
  reconstruction_loss: "bce"
  recon_weight: 1.0
  kl_weight: 1.0
  kl_c_weight: 1.0
  label_weight: 1.0

  # Training
  learning_rate: 0.001
  batch_size: 128
  max_epochs: 50
  patience: 10
  val_split: 0.1
  random_seed: 42
  grad_clip_norm: 5.0
  weight_decay: 0.0001
  dropout_rate: 0.2

  # Regularizers
  component_diversity_weight: -0.05
  dirichlet_alpha: null
  dirichlet_weight: 0.05

  # Optional features
  use_tau_classifier: false
  use_heteroscedastic_decoder: false
  top_m_gating: 0
  soft_embedding_warmup_epochs: 0
