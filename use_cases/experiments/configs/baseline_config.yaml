# ============================================================================
# Baseline Mixture Prior Configuration with τ-Classifier
# ============================================================================
# Baseline configuration for mixture prior SSVAE with τ-classifier.
# Uses convolutional architecture for MNIST and recommended hyperparameters.
#
# Key Features:
# - Convolutional encoder/decoder for image data
# - Mixture prior with K=10 components
# - τ-based latent-only classification
# - Component-aware decoder for functional specialization
# - Component diversity regularization (entropy reward)
#
# Usage:
#   poetry run python use_cases/experiments/run_experiment.py --config use_cases/experiments/configs/baseline_config.yaml
# ============================================================================

experiment:
  name: "baseline"                  # Short identifier for this experiment
  description: "Baseline mixture model with τ-classifier for MNIST"
  tags: ["baseline", "mixture", "tau-classifier", "conv"]

data:
  dataset: "mnist"                  # Dataset identifier (currently only "mnist" supported)
  num_samples: 2500                # Total samples to use from dataset
  num_labeled: 250                  # Number of labeled samples (semi-supervised)
  seed: 42                          # Random seed for data sampling

model:
  # ═════════════════════════════════════════════════════════════════════════
  # Problem Setup
  # ═════════════════════════════════════════════════════════════════════════
  num_classes: 10                   # Number of output classes
  latent_dim: 2                     # Latent space dimensionality (2 for visualization)
  hidden_dims: [256, 128, 64]       # MLP layer sizes (dense encoder only)

  # ═════════════════════════════════════════════════════════════════════════
  # Architecture Selection
  # ═════════════════════════════════════════════════════════════════════════
  encoder_type: "conv"              # Options: "conv" | "dense"
  decoder_type: "conv"              # Options: "conv" | "dense"
  classifier_type: "dense"          # Options: "dense" (only option currently)
  # input_hw: null                  # Optional: [height, width] override (default: infer from data)

  # ═════════════════════════════════════════════════════════════════════════
  # Loss Configuration
  # ═════════════════════════════════════════════════════════════════════════
  reconstruction_loss: "bce"        # Options: "bce" | "mse"
                                    #   "bce": Binary cross-entropy (recommended for MNIST)
                                    #   "mse": Mean squared error (for natural images)
  recon_weight: 1.0                 # Reconstruction loss weight (1.0 for BCE, 500 for MSE)
  kl_weight: 1.0                    # KL divergence weight (standard VAE β=1)
  label_weight: 1.0                 # Classification loss weight (currently unused)

  # ═════════════════════════════════════════════════════════════════════════
  # Optimizer & Training
  # ═════════════════════════════════════════════════════════════════════════
  learning_rate: 0.001              # Adam optimizer learning rate
  batch_size: 128                   # Samples per training batch
  max_epochs: 50                   # Maximum training epochs
  patience: 20                      # Early stopping patience (epochs without improvement)
  val_split: 0.1                    # Validation set fraction
  random_seed: 42                   # Random seed for reproducibility
  grad_clip_norm: 5.0               # Gradient clipping threshold (null to disable)
  weight_decay: 0.0001              # L2 weight decay (AdamW-style)
  dropout_rate: 0.1                 # Dropout rate in classifier

  # ═════════════════════════════════════════════════════════════════════════
  # Monitoring & Early Stopping
  # ═════════════════════════════════════════════════════════════════════════
  monitor_metric: "loss"        # Options: "val_loss" | "loss" | "classification_loss"
                                    # Metric used for early stopping decisions

  # ═════════════════════════════════════════════════════════════════════════
  # Contrastive Learning (Optional)
  # ═════════════════════════════════════════════════════════════════════════
  use_contrastive: false            # Enable contrastive loss term
  contrastive_weight: 0.0           # Contrastive loss weight (when enabled)

  # ═════════════════════════════════════════════════════════════════════════
  # Prior Type & Mixture Configuration
  # ═════════════════════════════════════════════════════════════════════════
  prior_type: "mixture"             # Options: "standard" | "mixture"
                                    #   "standard": Single Gaussian N(0,I)
                                    #   "mixture": Mixture of Gaussians (GMM prior)
  num_components: 10                # Number of mixture components (K, mixture prior only)

  # Mixture-specific loss weights
  kl_c_weight: 1.0                  # KL(q(c|x) || π) weight (mixture prior only)
  kl_c_anneal_epochs: 50            # Epochs to anneal kl_c_weight from 0→1 (0 = no annealing)

  # Mixture prior regularization
  dirichlet_alpha: null              # Dirichlet-MAP prior strength (null = disabled Dirichlet regularization, try 2.0-5.0)
  dirichlet_weight: 0.05             # Dirichlet-MAP penalty weight
  
  component_diversity_weight: -0.10 # Usage diversity regularization
                                    #   NEGATIVE (e.g., -0.05): Encourage diversity (RECOMMENDED)
                                    #   POSITIVE: Discourage diversity (causes collapse)
                                    #   0: No regularization

  # ═════════════════════════════════════════════════════════════════════════
  # Component-Aware Decoder (Mixture Prior Feature)
  # ═════════════════════════════════════════════════════════════════════════
  use_component_aware_decoder: true # Enable component-aware decoding
                                    #   true: Separate processing for z and e_c (recommended)
                                    #   false: Standard decoder (ignores components)
  component_embedding_dim: 8        # Component embedding dimensionality
                                    #   Recommended: 4-16 (small to avoid overwhelming latent)
  top_m_gating: 0                   # Top-M component gating (0 = use all components)
                                    #   >0: Only use top-M components by responsibility
                                    #   Reduces computation for large K
  soft_embedding_warmup_epochs: 10  # Soft-weighted embedding warmup epochs
                                    #   >0: Use soft weights for N initial epochs
                                    #   0: Hard sampling from start

  # ═════════════════════════════════════════════════════════════════════════
  # τ-Classifier (Latent-Only Classification)
  # ═════════════════════════════════════════════════════════════════════════
  use_tau_classifier: false          # Use τ-based classification (mixture prior only)
                                    #   true: Latent-only classifier using τ counts
                                    #   false: Standard classifier head
                                    #   NOTE: Requires num_components >= num_classes
  tau_smoothing_alpha: 1.0          # Laplace smoothing prior (α₀) for τ counts
                                    #   Prevents zero probabilities for unseen pairs

  # ═════════════════════════════════════════════════════════════════════════
  # Heteroscedastic Decoder (Aleatoric Uncertainty)
  # ═════════════════════════════════════════════════════════════════════════
  use_heteroscedastic_decoder: true # Learn per-image variance σ(x)
                                    #   true: Decoder outputs (mean, sigma) for uncertainty
                                    #   false: Standard decoder (mean only)
  sigma_min: 0.05                   # Minimum allowed σ (prevents variance collapse)
  sigma_max: 0.5                    # Maximum allowed σ (prevents variance explosion)

  # ═════════════════════════════════════════════════════════════════════════
  # Mixture Tracking & Diagnostics
  # ═════════════════════════════════════════════════════════════════════════
  mixture_history_log_every: 1      # Track π and usage every N epochs
                                    #   1: Every epoch (detailed, more I/O)
                                    #   5-10: Coarser tracking (less overhead)
