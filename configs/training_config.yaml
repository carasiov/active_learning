# ============================================================================
# SSVAE Training Configuration - Complete Reference
# ============================================================================
# All hyperparameters in one place under models section.
# No inheritance, no merging - just specify everything directly.
#
# Usage:
#   python scripts/compare_models.py --config configs/training_config.yaml
# ============================================================================

description: "Complete SSVAE training configuration"

# ============================================================================
# DATA CONFIGURATION
# ============================================================================
data:
  num_samples: 5000      # Total training samples (max: 60000 for MNIST)
  num_labeled: 50        # Labeled samples for semi-supervised learning
  epochs: 30             # Training epochs
  seed: 42               # Random seed for reproducibility

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
# All SSVAEConfig hyperparameters specified directly.
# Add more models here to compare multiple configurations.

models:
  MyModel:
    # ---- Architecture ----
    encoder_type: "dense"           # "dense" | "conv"
    decoder_type: "dense"           # "dense" | "conv"
    classifier_type: "dense"        # "dense" (only option)
    latent_dim: 2                   # Latent space dimensionality
    hidden_dims: [256, 128, 64]     # Hidden layer sizes (dense networks)
    input_hw: null                  # Optional [height, width] override
    
    # ---- Loss Configuration ----
    reconstruction_loss: "mse"      # "mse" | "bce"
                                    # mse: natural images (weight ~500)
                                    # bce: binary images (weight ~1.0)
    recon_weight: 500.0             # Reconstruction loss weight
    kl_weight: 5.0                  # KL divergence weight
    label_weight: 0.0               # Classification loss weight (unused)
    
    # ---- Prior Configuration ----
    prior_type: "standard"          # "standard" | "mixture"
    num_components: 10              # Mixture components (mixture prior only)
    component_kl_weight: 0.1        # Component KL weight (mixture prior only)
    
    # ---- Optimizer & Training ----
    learning_rate: 0.001            # Adam learning rate
    batch_size: 128                 # Training batch size
    max_epochs: 300                 # Maximum epochs
    patience: 50                    # Early stopping patience (epochs)
    val_split: 0.1                  # Validation fraction (0.0-1.0)
    random_seed: 42                 # Initialization seed
    
    # ---- Regularization ----
    grad_clip_norm: 1.0             # Gradient clipping (null to disable)
    weight_decay: 0.0001            # L2 weight decay
    dropout_rate: 0.2               # Dropout in classifier
    
    # ---- Training Control ----
    monitor_metric: "classification_loss"  # Early stopping metric
    
    # ---- Advanced Features ----
    use_contrastive: false          # Enable contrastive loss
    contrastive_weight: 0.0         # Contrastive loss weight
    xla_flags: null                 # XLA_FLAGS override

# ============================================================================
# CONFIGURATION EXAMPLES
# ============================================================================

# # Quick Test (Dense, Small Dataset)
# data:
#   num_samples: 1000
#   num_labeled: 30
#   epochs: 20
# model:
#   encoder_type: "dense"
#   decoder_type: "dense"
#   latent_dim: 2
#   hidden_dims: [128, 64]
#   max_epochs: 20
#   batch_size: 256

# # Convolutional MNIST (BCE Loss)
# data:
#   num_samples: 50000
#   num_labeled: 500
#   epochs: 50
# model:
#   encoder_type: "conv"
#   decoder_type: "conv"
#   latent_dim: 2
#   reconstruction_loss: "bce"
#   recon_weight: 1.0
#   max_epochs: 50
#   batch_size: 512

# # Mixture Prior Experiment (Unsupervised)
# data:
#   num_samples: 50000
#   num_labeled: 0
#   epochs: 300
# model:
#   prior_type: "mixture"
#   num_components: 10
#   component_kl_weight: 0.1
#   latent_dim: 2
#   max_epochs: 300

# # High-Dimensional Latent Space
# model:
#   latent_dim: 10
#   prior_type: "mixture"
#   num_components: 10
#   hidden_dims: [512, 256, 128]

# ============================================================================
# PARAMETER GUIDANCE
# ============================================================================
#
# Reconstruction Loss:
#   - MSE: Continuous pixel values, use recon_weight ~500
#   - BCE: Binary/binarized images, use recon_weight ~1.0
#
# Prior Type:
#   - Standard: Simple Gaussian N(0,I), faster, good baseline
#   - Mixture: GMM prior, better clustering, needs more data
#
# Architecture:
#   - Dense: Flexible, configurable layers, works for any input
#   - Conv: MNIST-specific (28Ã—28), hardcoded layers, more efficient
#
# Semi-supervised:
#   - num_labeled = 0: Fully unsupervised VAE
#   - num_labeled > 0: Semi-supervised SSVAE
#   - Keep num_labeled << num_samples for true semi-supervision
#
# Training:
#   - Increase batch_size for speed (requires more GPU memory)
#   - Set patience = max_epochs to disable early stopping
#   - Use smaller datasets for quick iteration/debugging
#
# GPU Issues:
#   - Reduce batch_size if OOM
#   - Set JAX_PLATFORMS=cpu for CPU execution
#   - Clear cache: rm -rf ~/.cache/jax*
# ============================================================================
